{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVMT6r04qKtj",
        "outputId": "89d955d2-ebcf-4aa6-bb29-11d068e30c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PrevExchangeRate  PrevExchangeRate2  PrevExchangeRate3      Date\n",
            "3             20700.0            20600.0            21600.0  738218.0\n",
            "4             20750.0            20700.0            20600.0  738219.0\n",
            "5             20750.0            20750.0            20700.0  738220.0\n",
            "6             20775.0            20750.0            20750.0  738221.0\n",
            "7             21700.0            20775.0            20750.0  738222.0\n",
            "..                ...                ...                ...       ...\n",
            "761           89600.0            89600.0            89600.0  738976.0\n",
            "762           89600.0            89600.0            89600.0  738977.0\n",
            "763           89600.0            89600.0            89600.0  738978.0\n",
            "764           89600.0            89600.0            89600.0  738979.0\n",
            "765           89600.0            89600.0            89600.0  738980.0\n",
            "\n",
            "[763 rows x 4 columns]\n",
            "3      20750\n",
            "4      20750\n",
            "5      20775\n",
            "6      21700\n",
            "7      21700\n",
            "       ...  \n",
            "761    89600\n",
            "762    89600\n",
            "763    89600\n",
            "764    89600\n",
            "765    89600\n",
            "Name: ExchangeRate, Length: 763, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#CHATGPT helped me modify this code to work. I had problems with the shape later on in the code\n",
        "\n",
        "data = pd.read_excel(\"exchange_rate_data.xlsx\")\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "dateData = data['Date']\n",
        "\n",
        "data['PrevExchangeRate'] = data['ExchangeRate'].shift(1)\n",
        "data['PrevExchangeRate2'] = data['ExchangeRate'].shift(2)\n",
        "data['PrevExchangeRate3'] = data['ExchangeRate'].shift(3)\n",
        "\n",
        "data['Date'] = data['Date'].apply(lambda x: x.toordinal()).astype(float)\n",
        "\n",
        "X = data[['PrevExchangeRate', 'PrevExchangeRate2', 'PrevExchangeRate3', 'Date']][3:]\n",
        "y = data['ExchangeRate'][3:]\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, shuffle = False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle = False)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_val = y_val.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_val_reshaped = np.reshape(X_val_scaled, (X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "print(X_train_reshaped)\n",
        "print(y_train_scaled)"
      ],
      "metadata": {
        "id": "6qZok8h7qN6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1119d740-61f3-407a-8e10-030755e4633e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-1.42265874 -1.42140448 -1.38309007 -1.72952779]]\n",
            "\n",
            " [[-1.42096379 -1.41801752 -1.41693304 -1.72447807]]\n",
            "\n",
            " [[-1.42096379 -1.41632404 -1.41354874 -1.71942836]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.91298153  0.91559599  0.91823134  1.71942836]]\n",
            "\n",
            " [[ 0.91298153  0.91559599  0.91823134  1.72447807]]\n",
            "\n",
            " [[ 0.91298153  0.91559599  0.91823134  1.72952779]]]\n",
            "[[-1.42561426]\n",
            " [-1.42561426]\n",
            " [-1.42476605]\n",
            " [-1.39338206]\n",
            " [-1.39338206]\n",
            " [-1.32213192]\n",
            " [-1.32382836]\n",
            " [-1.37302488]\n",
            " [-1.35860521]\n",
            " [-1.36454272]\n",
            " [-1.36963201]\n",
            " [-1.38320347]\n",
            " [-1.3687838 ]\n",
            " [-1.37302488]\n",
            " [-1.33315873]\n",
            " [-1.34588197]\n",
            " [-1.3526677 ]\n",
            " [-1.34079267]\n",
            " [-1.32467657]\n",
            " [-1.33400695]\n",
            " [-1.31195333]\n",
            " [-1.30856047]\n",
            " [-1.3102569 ]\n",
            " [-1.30856047]\n",
            " [-1.30516761]\n",
            " [-1.30007831]\n",
            " [-1.3153462 ]\n",
            " [-1.32552479]\n",
            " [-1.32552479]\n",
            " [-1.32552479]\n",
            " [-1.32722122]\n",
            " [-1.32552479]\n",
            " [-1.32552479]\n",
            " [-1.32552479]\n",
            " [-1.3153462 ]\n",
            " [-1.32891765]\n",
            " [-1.32213192]\n",
            " [-1.32213192]\n",
            " [-1.31195333]\n",
            " [-1.31195333]\n",
            " [-1.29159615]\n",
            " [-1.29498902]\n",
            " [-1.29498902]\n",
            " [-1.29498902]\n",
            " [-1.29498902]\n",
            " [-1.28820329]\n",
            " [-1.28820329]\n",
            " [-1.26445324]\n",
            " [-1.26445324]\n",
            " [-1.24748893]\n",
            " [-1.24748893]\n",
            " [-1.23052461]\n",
            " [-1.23052461]\n",
            " [-1.20677456]\n",
            " [-1.21356029]\n",
            " [-1.21356029]\n",
            " [-1.22882818]\n",
            " [-1.23052461]\n",
            " [-1.2356139 ]\n",
            " [-1.2356139 ]\n",
            " [-1.23391747]\n",
            " [-1.23391747]\n",
            " [-1.23391747]\n",
            " [-1.23391747]\n",
            " [-1.23391747]\n",
            " [-1.23052461]\n",
            " [-1.23052461]\n",
            " [-1.22713174]\n",
            " [-1.22713174]\n",
            " [-1.22713174]\n",
            " [-1.22713174]\n",
            " [-1.22713174]\n",
            " [-1.22713174]\n",
            " [-1.21356029]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.14570302]\n",
            " [-1.04730997]\n",
            " [-1.11177438]\n",
            " [-1.11177438]\n",
            " [-1.06088143]\n",
            " [-1.06088143]\n",
            " [-1.06088143]\n",
            " [-1.16266734]\n",
            " [-1.21356029]\n",
            " [-1.21356029]\n",
            " [-1.21356029]\n",
            " [-1.21356029]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.19320311]\n",
            " [-1.19320311]\n",
            " [-1.19320311]\n",
            " [-1.19320311]\n",
            " [-1.19659597]\n",
            " [-1.19659597]\n",
            " [-1.18302452]\n",
            " [-1.18302452]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.17963165]\n",
            " [-1.15927447]\n",
            " [-1.15248874]\n",
            " [-1.15248874]\n",
            " [-1.15248874]\n",
            " [-1.15248874]\n",
            " [-1.17963165]\n",
            " [-1.18302452]\n",
            " [-1.18302452]\n",
            " [-1.18302452]\n",
            " [-1.1660602 ]\n",
            " [-1.1660602 ]\n",
            " [-1.1660602 ]\n",
            " [-1.1660602 ]\n",
            " [-1.16266734]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.14231015]\n",
            " [-1.14231015]\n",
            " [-1.13891729]\n",
            " [-1.13891729]\n",
            " [-1.13891729]\n",
            " [-1.13722086]\n",
            " [-1.1287387 ]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13722086]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.13043513]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.08463147]\n",
            " [-1.06766716]\n",
            " [-1.06766716]\n",
            " [-1.06766716]\n",
            " [-1.06088143]\n",
            " [-1.06088143]\n",
            " [-0.99302416]\n",
            " [-0.99302416]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.95909552]\n",
            " [-0.87427393]\n",
            " [-0.85730961]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.87427393]\n",
            " [-0.85730961]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.82338098]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.78775591]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.9421312 ]\n",
            " [-0.94891693]\n",
            " [-0.90820257]\n",
            " [-0.90820257]\n",
            " [-0.88105966]\n",
            " [-0.88105966]\n",
            " [-0.88105966]\n",
            " [-0.88105966]\n",
            " [-0.88105966]\n",
            " [-0.87427393]\n",
            " [-0.84204173]\n",
            " [-0.84882745]\n",
            " [-0.85730961]\n",
            " [-0.84713102]\n",
            " [-0.84713102]\n",
            " [-0.83695243]\n",
            " [-0.78436305]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.77079159]\n",
            " [-0.77282731]\n",
            " [-0.78775591]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.78945234]\n",
            " [-0.77588089]\n",
            " [-0.77588089]\n",
            " [-0.78436305]\n",
            " [-0.78436305]\n",
            " [-0.79454164]\n",
            " [-0.79454164]\n",
            " [-0.79454164]\n",
            " [-0.79454164]\n",
            " [-0.77927375]\n",
            " [-0.77588089]\n",
            " [-0.77248802]\n",
            " [-0.76909516]\n",
            " [-0.7555237 ]\n",
            " [-0.74534511]\n",
            " [-0.73855939]\n",
            " [-0.73855939]\n",
            " [-0.73855939]\n",
            " [-0.73855939]\n",
            " [-0.7283808 ]\n",
            " [-0.71820221]\n",
            " [-0.70802361]\n",
            " [-0.70802361]\n",
            " [-0.70463075]\n",
            " [-0.69784502]\n",
            " [-0.69445216]\n",
            " [-0.68766643]\n",
            " [-0.67748784]\n",
            " [-0.67070212]\n",
            " [-0.66221996]\n",
            " [-0.66391639]\n",
            " [-0.64355921]\n",
            " [-0.62998775]\n",
            " [-0.62320202]\n",
            " [-0.60963057]\n",
            " [-0.57570193]\n",
            " [-0.57230907]\n",
            " [-0.59605912]\n",
            " [-0.58927339]\n",
            " [-0.56213048]\n",
            " [-0.51123753]\n",
            " [-0.67070212]\n",
            " [-0.63338062]\n",
            " [-0.63338062]\n",
            " [-0.70463075]\n",
            " [-0.69445216]\n",
            " [-0.69445216]\n",
            " [-0.70123789]\n",
            " [-0.69445216]\n",
            " [-0.66730925]\n",
            " [-0.6537378 ]\n",
            " [-0.62659489]\n",
            " [-0.66052352]\n",
            " [-0.67070212]\n",
            " [-0.58248766]\n",
            " [-0.56552334]\n",
            " [-0.51463039]\n",
            " [-0.50784466]\n",
            " [-0.47391603]\n",
            " [-0.4671303 ]\n",
            " [-0.4671303 ]\n",
            " [-0.50784466]\n",
            " [-0.49088034]\n",
            " [-0.4671303 ]\n",
            " [-0.4671303 ]\n",
            " [-0.41454091]\n",
            " [-0.38909444]\n",
            " [-0.3924873 ]\n",
            " [-0.37213012]\n",
            " [-0.26695135]\n",
            " [-0.17195117]\n",
            " [-0.01587944]\n",
            " [-0.19570121]\n",
            " [-0.11766535]\n",
            " [-0.16177258]\n",
            " [-0.1685583 ]\n",
            " [-0.16177258]\n",
            " [ 0.04179924]\n",
            " [ 0.0078706 ]\n",
            " [ 0.0078706 ]\n",
            " [ 0.0078706 ]\n",
            " [-0.02605803]\n",
            " [ 0.0078706 ]\n",
            " [ 0.04179924]\n",
            " [ 0.04179924]\n",
            " [ 0.04179924]\n",
            " [ 0.05876356]\n",
            " [ 0.05876356]\n",
            " [ 0.05876356]\n",
            " [ 0.12662083]\n",
            " [ 0.24537105]\n",
            " [ 0.41501423]\n",
            " [ 0.58465741]\n",
            " [ 0.61858605]\n",
            " [ 0.54055019]\n",
            " [ 0.53376446]\n",
            " [ 0.63555037]\n",
            " [ 0.63555037]\n",
            " [ 0.58465741]\n",
            " [ 0.61858605]\n",
            " [ 0.61858605]\n",
            " [ 0.63555037]\n",
            " [ 0.61858605]\n",
            " [ 0.75430059]\n",
            " [ 0.83912218]\n",
            " [ 0.55072878]\n",
            " [ 0.55072878]\n",
            " [ 0.55072878]\n",
            " [ 0.56769309]\n",
            " [ 0.56769309]\n",
            " [ 0.56769309]\n",
            " [ 0.57447882]\n",
            " [ 0.57447882]\n",
            " [ 0.72037196]\n",
            " [ 0.92394377]\n",
            " [ 0.97483673]\n",
            " [ 0.99180104]\n",
            " [ 1.12751559]\n",
            " [ 1.29715877]\n",
            " [ 1.36501604]\n",
            " [ 1.46680195]\n",
            " [ 1.53465922]\n",
            " [ 1.56858785]\n",
            " [ 1.63644513]\n",
            " [ 2.00966012]\n",
            " [ 1.43287331]\n",
            " [ 1.53465922]\n",
            " [ 1.43287331]\n",
            " [ 1.50073058]\n",
            " [ 1.53465922]\n",
            " [ 1.50073058]\n",
            " [ 1.46680195]\n",
            " [ 1.46680195]\n",
            " [ 1.48376626]\n",
            " [ 1.50073058]\n",
            " [ 1.46680195]\n",
            " [ 1.48376626]\n",
            " [ 1.34805172]\n",
            " [ 1.17840854]\n",
            " [ 1.19537286]\n",
            " [ 1.16144422]\n",
            " [ 1.19537286]\n",
            " [ 1.17840854]\n",
            " [ 1.12751559]\n",
            " [ 1.14447991]\n",
            " [ 1.14447991]\n",
            " [ 1.16144422]\n",
            " [ 1.14447991]\n",
            " [ 1.14447991]\n",
            " [ 1.14447991]\n",
            " [ 1.14447991]\n",
            " [ 1.16144422]\n",
            " [ 1.16144422]\n",
            " [ 1.16144422]\n",
            " [ 1.16144422]\n",
            " [ 1.13769418]\n",
            " [ 1.13769418]\n",
            " [ 1.14447991]\n",
            " [ 1.14447991]\n",
            " [ 1.1546585 ]\n",
            " [ 1.1546585 ]\n",
            " [ 1.1546585 ]\n",
            " [ 1.16144422]\n",
            " [ 1.1546585 ]\n",
            " [ 1.1546585 ]\n",
            " [ 1.1546585 ]\n",
            " [ 1.1546585 ]\n",
            " [ 1.14447991]\n",
            " [ 1.13599775]\n",
            " [ 1.13769418]\n",
            " [ 1.13769418]\n",
            " [ 1.14278347]\n",
            " [ 1.12751559]\n",
            " [ 1.12242629]\n",
            " [ 1.11394413]\n",
            " [ 1.09697981]\n",
            " [ 1.08510479]\n",
            " [ 1.07322977]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.07322977]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.06983691]\n",
            " [ 1.07322977]\n",
            " [ 1.06983691]\n",
            " [ 1.05287259]\n",
            " [ 1.05287259]\n",
            " [ 1.05287259]\n",
            " [ 1.03930113]\n",
            " [ 1.01894395]\n",
            " [ 1.01894395]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.03251541]\n",
            " [ 1.03081897]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.01894395]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 1.02572968]\n",
            " [ 0.99180104]\n",
            " [ 1.00197963]\n",
            " [ 1.0053725 ]\n",
            " [ 1.00197963]\n",
            " [ 0.95787241]\n",
            " [ 0.96465813]\n",
            " [ 0.95787241]\n",
            " [ 0.95787241]\n",
            " [ 0.95787241]\n",
            " [ 0.95787241]\n",
            " [ 0.95787241]\n",
            " [ 0.968051  ]\n",
            " [ 0.968051  ]\n",
            " [ 0.968051  ]\n",
            " [ 0.968051  ]\n",
            " [ 0.968051  ]\n",
            " [ 0.96465813]\n",
            " [ 1.21233718]\n",
            " [ 0.97483673]\n",
            " [ 0.98162245]\n",
            " [ 0.98162245]\n",
            " [ 0.97483673]\n",
            " [ 0.98162245]\n",
            " [ 1.02572968]\n",
            " [ 1.00876536]\n",
            " [ 1.00197963]\n",
            " [ 1.00197963]\n",
            " [ 1.00197963]\n",
            " [ 0.98501532]\n",
            " [ 0.92394377]\n",
            " [ 0.90697945]\n",
            " [ 0.89001514]\n",
            " [ 0.90697945]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89680086]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89680086]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89680086]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.90019373]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.90019373]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.89001514]\n",
            " [ 0.90358659]\n",
            " [ 0.90697945]\n",
            " [ 0.90867589]\n",
            " [ 0.90867589]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91546161]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91206875]\n",
            " [ 0.91206875]\n",
            " [ 0.91206875]\n",
            " [ 0.91546161]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91376518]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]\n",
            " [ 0.91037232]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(50, input_shape=(1, X_train_scaled.shape[1])),\n",
        "    tf.keras.layers.Dense(30, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=15, verbose=1, validation_data=(X_val_reshaped, y_val_scaled))\n",
        "\n",
        "loss = model.evaluate(X_test_reshaped, y_test_scaled)\n",
        "print(f'Loss on test set: {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDW5XTYkIQvs",
        "outputId": "a5a6dd8c-9f3b-454f-8d19-3586d0045ab3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 1.1476 - val_loss: 0.0049\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8534 - val_loss: 3.0513e-04\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6186 - val_loss: 8.7734e-04\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4368 - val_loss: 0.0056\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2967 - val_loss: 0.0124\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1957 - val_loss: 0.0193\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.0251\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0781 - val_loss: 0.0287\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.0293\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.0283\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0252\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0220\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0187\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0156\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0132\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0052 - val_loss: 0.0110\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0093\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0080\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0072\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0061\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0055\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0049\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0030\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0027\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0024\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0022\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0019\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0015\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0013\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0010\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 9.6766e-04\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 8.7752e-04\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 7.4037e-04\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 6.4485e-04\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 6.4226e-04\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 5.4714e-04\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 5.5955e-04\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 4.8658e-04\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 4.5943e-04\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 4.1181e-04\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 3.5967e-04\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 3.3004e-04\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 3.0612e-04\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.4962e-04\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.6080e-04\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.8261e-04\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.6588e-04\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 3.0171e-04\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.7428e-04\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 2.3762e-04\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.1006e-04\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.2805e-04\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.0228e-04\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 2.5099e-04\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 1.7483e-04\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.1886e-04\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.4249e-04\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.0360e-04\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 1.6091e-04\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 2.0536e-04\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0031 - val_loss: 1.3658e-04\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.4565e-04\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.9629e-04\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.4425e-04\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.4517e-04\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 7.4022e-05\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.6801e-04\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.7531e-04\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.1873e-04\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 2.4133e-04\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 1.4234e-04\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 9.0723e-05\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 1.6555e-04\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 6.3979e-05\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 1.6293e-04\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 1.8837e-04\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 1.1321e-04\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 1.1394e-04\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 1.4683e-04\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 1.3368e-04\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 9.7148e-05\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 1.2853e-04\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 4.8026e-05\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 1.3352e-04\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 1.8298e-04\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 1.2515e-04\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 1.6342e-04\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 1.5414e-04\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 2.5321e-05\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 1.3691e-04\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 9.6107e-05\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 2.2260e-04\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 1.5904e-04\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 6.2334e-05\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 1.1748e-04\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.5934e-05\n",
            "Loss on test set: 7.593353802803904e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_exchange_rate(date, prevRate1, prevRae2, prevRate3):\n",
        "    date_ordinal = pd.to_datetime(date).toordinal()\n",
        "    date_scaled = scaler.transform([[prevRate1, prevRae2, prevRate3, date_ordinal]])\n",
        "    date_reshaped = np.reshape(date_scaled, (1, 1, 4))\n",
        "    predicted_rate = model.predict(date_reshaped, verbose = 0)\n",
        "    return scaler_y.inverse_transform(predicted_rate[0])[0][0]\n",
        "\n",
        "future_date = '4/5/2024'\n",
        "prevRate1 = y[len(y)-1]\n",
        "prevRate2 = y[len(y)-2]\n",
        "prevRate3 = y[len(y)-3]\n",
        "predicted_rate = predict_exchange_rate(future_date, prevRate1, prevRate2, prevRate3)\n",
        "print(f'Predicted exchange rate for {future_date}: {predicted_rate}')\n",
        "\n",
        "future_date = '4/6/2024'\n",
        "predicted_rate2 = predict_exchange_rate(future_date, prevRate1, prevRate2, prevRate3)\n",
        "print(f'Predicted exchange rate for {future_date}: {predicted_rate2}')\n",
        "\n",
        "future_date = '4/7/2024'\n",
        "predicted_rate3 = predict_exchange_rate(future_date, prevRate1, prevRate2, prevRate3)\n",
        "print(f'Predicted exchange rate for {future_date}: {predicted_rate3}')\n",
        "\n",
        "future_date = '4/8/2024'\n",
        "predicted_rate4 = predict_exchange_rate(future_date, prevRate1, prevRate2, prevRate3)\n",
        "print(f'Predicted exchange rate for {future_date}: {predicted_rate4}')\n",
        "\n",
        "future_date = '4/9/2024'\n",
        "predicted_rate5 = predict_exchange_rate(future_date, prevRate1, prevRate2, prevRate3)\n",
        "print(f'Predicted exchange rate for {future_date}: {predicted_rate5}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFD5NJnN1Ncb",
        "outputId": "5aa5a780-0346-4a79-bc11-cde5ea13ab12"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted exchange rate for 4/5/2024: 89820.25\n",
            "Predicted exchange rate for 4/6/2024: 89818.3828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted exchange rate for 4/7/2024: 89816.5\n",
            "Predicted exchange rate for 4/8/2024: 89814.6171875\n",
            "Predicted exchange rate for 4/9/2024: 89812.7265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model.save(\"exchangeRateModel\" + \".h5\")\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(scaler_y, 'scaler_y.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RiGzDN_1Y_z",
        "outputId": "6f4d8b92-0e54-405d-e709-cd7e7b433e53"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler_y.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BzDBJPCLG8jv"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}